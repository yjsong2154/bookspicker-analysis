import os
import sys
import random
import json
import time
from dotenv import load_dotenv

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules import converter, splitter, tagger, aggregator, vectorizer

def main():
    # 1. í™˜ê²½ ì„¤ì •
    load_dotenv() # .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë˜ëŠ” ìƒìœ„ ë””ë ‰í† ë¦¬ íƒìƒ‰)
    
    if not os.getenv("GMS_KEY"):
        print("âš ï¸  Warning: GMS_KEY not found in environment variables. Tagging might fail.")
        print("Please ensure .env file exists in this directory or parent directory.")

    base_dir = os.path.dirname(os.path.abspath(__file__))
    input_dir = os.path.join(base_dir, "input")
    output_dir = os.path.join(base_dir, "output")
    
    txt_output_dir = os.path.join(output_dir, "txt")
    chunks_output_dir = os.path.join(output_dir, "chunks")
    tags_output_dir = os.path.join(output_dir, "tags")
    final_tags_output_dir = os.path.join(output_dir, "final_tags") # ìµœì¢… íƒœê·¸ ì €ì¥ í´ë”
    
    # ë²¡í„° ê´€ë ¨ í´ë”
    chunks_vec_output_dir = os.path.join(output_dir, "chunks_vec")
    vecs_output_dir = os.path.join(output_dir, "vecs")
    final_vec_output_dir = os.path.join(output_dir, "final_vec")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(input_dir, exist_ok=True)
    os.makedirs(txt_output_dir, exist_ok=True)
    os.makedirs(chunks_output_dir, exist_ok=True)
    os.makedirs(tags_output_dir, exist_ok=True)
    os.makedirs(final_tags_output_dir, exist_ok=True)
    os.makedirs(chunks_vec_output_dir, exist_ok=True)
    os.makedirs(vecs_output_dir, exist_ok=True)
    os.makedirs(final_vec_output_dir, exist_ok=True)

    # 2. EPUB íŒŒì¼ ëª©ë¡ ìŠ¤ìº”
    epub_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.epub')]
    
    if not epub_files:
        print(f"â„¹ï¸  No .epub files found in {input_dir}")
        print("Please place .epub files in the 'input' directory and run again.")
        return

    print(f"ğŸ“š Found {len(epub_files)} epub files. Starting processing...")

    for epub_file in epub_files:
        print(f"\nğŸš€ Processing: {epub_file}")
        file_name_no_ext = os.path.splitext(epub_file)[0]
        epub_path = os.path.join(input_dir, epub_file)
        
        # --- Step 1: EPUB to TXT ---
        txt_filename = f"{file_name_no_ext}_text.txt"
        txt_path = os.path.join(txt_output_dir, txt_filename)
        
        print(f"  [1/5] Converting to TXT: {txt_filename}")
        try:
            converter.convert_epub_to_txt(epub_path, txt_path)
        except Exception as e:
            print(f"  âŒ Failed to convert {epub_file}: {e}")
            continue

        # --- Step 2: TXT to Chunks (Taggingìš©) ---
        print(f"  [2/5] Splitting into Chunks (for Tagging)...")
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            chunks = splitter.split_into_chunks(text_content) # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            
            # ì²­í¬ ì €ì¥ í´ë”: (ì›ë˜ì œëª©)_chunks
            book_chunks_dir = os.path.join(chunks_output_dir, f"{file_name_no_ext}_chunks")
            # íŒŒì¼ prefix: (ì›ë˜ì œëª©)_chunk
            chunk_prefix = f"{file_name_no_ext}_chunk"
            
            saved_chunk_paths = splitter.save_chunks(chunks, book_chunks_dir, chunk_prefix)
            print(f"    -> Created {len(saved_chunk_paths)} chunks in {book_chunks_dir}")
            
        except Exception as e:
            print(f"  âŒ Failed to split chunks for {epub_file}: {e}")
            continue

        # --- Step 3: Sampling & Tagging ---
        print(f"  [3/5] Sampling and Tagging...")
        
        # íƒœê·¸ ì €ì¥ í´ë”: (ì›ë˜ì œëª©)_tags
        book_tags_dir = os.path.join(tags_output_dir, f"{file_name_no_ext}_tags")
        os.makedirs(book_tags_dir, exist_ok=True)

        # ì „ì²´ ì²­í¬ ì¤‘ 5ê°œë¥¼ ê· ì¼í•˜ê²Œ ëœë¤ ì„ íƒ
        target_sample_count = 5
        selected_indices = []
        
        if len(chunks) <= target_sample_count:
            selected_indices = list(range(len(chunks)))
        else:
            # ì „ì²´ë¥¼ 5ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ê°„ì—ì„œ í•˜ë‚˜ì”© ì„ íƒ
            segment_size = len(chunks) / target_sample_count
            for i in range(target_sample_count):
                start_idx = int(i * segment_size)
                end_idx = int((i + 1) * segment_size)
                
                if i == target_sample_count - 1:
                    end_idx = len(chunks)
                
                if start_idx < end_idx:
                    selected_idx = random.randint(start_idx, end_idx - 1)
                    selected_indices.append(selected_idx)
        
        print(f"    -> Selected {len(selected_indices)} chunks for tagging (out of {len(chunks)})")
        
        for i, idx in enumerate(selected_indices):
            chunk_text = chunks[idx]
            tag_filename = f"{file_name_no_ext}_tag_{i+1:02d}.json"
            tag_path = os.path.join(book_tags_dir, tag_filename)
            
            if os.path.exists(tag_path):
                 print(f"    -> Tagging chunk {idx+1}/{len(chunks)} as {tag_filename}... (Skipping, file exists)")
                 continue

            print(f"    -> Tagging chunk {idx+1}/{len(chunks)} as {tag_filename}...", end="", flush=True)
            
            tags = tagger.tag_chunk_with_gpt(chunk_text)
            
            if tags:
                with open(tag_path, 'w', encoding='utf-8') as f:
                    json.dump(tags, f, ensure_ascii=False, indent=2)
                print(" Done.")
            else:
                print(" Failed.")
            
            time.sleep(0.5)

        # --- Step 4: Tag Aggregation ---
        print(f"  [4/5] Aggregating Tags...")
        try:
            aggregator.aggregate_tags(book_tags_dir, final_tags_output_dir, file_name_no_ext)
        except Exception as e:
            print(f"  âŒ Failed to aggregate tags: {e}")

        # --- Step 5: Vector Processing ---
        print(f"  [5/5] Processing Vectors...")
        try:

            # 1) í…ìŠ¤íŠ¸ ì¬ë¶„í•  (Vectorìš©, chunk_size=500)
            # Note: 1600 words exceeded 8192 token limit (approx 12k tokens). 
            # Adjusted to 500 words (~4k tokens) to be safe.
            print("    -> Re-splitting text for vectors (chunk_size=500)...")
            vec_chunks = splitter.split_into_chunks(text_content, chunk_size=500)
            
            # ë²¡í„° ì²­í¬ ì €ì¥ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ìš”ì²­ì— í¬í•¨ë¨)
            book_vec_chunks_dir = os.path.join(chunks_vec_output_dir, f"{file_name_no_ext}_vec_chunks")
            vec_chunk_prefix = f"{file_name_no_ext}_vec_chunk"
            splitter.save_chunks(vec_chunks, book_vec_chunks_dir, vec_chunk_prefix)
            print(f"    -> Created {len(vec_chunks)} vector chunks in {book_vec_chunks_dir}")

            # 2) ìƒ˜í”Œë§ (5ê°œ ê· ì¼)
            vec_selected_indices = []
            if len(vec_chunks) <= target_sample_count:
                vec_selected_indices = list(range(len(vec_chunks)))
            else:
                segment_size = len(vec_chunks) / target_sample_count
                for i in range(target_sample_count):
                    start_idx = int(i * segment_size)
                    end_idx = int((i + 1) * segment_size)
                    if i == target_sample_count - 1:
                        end_idx = len(vec_chunks)
                    if start_idx < end_idx:
                        selected_idx = random.randint(start_idx, end_idx - 1)
                        vec_selected_indices.append(selected_idx)
            
            print(f"    -> Selected {len(vec_selected_indices)} chunks for vectorization")

            # 3) ë²¡í„° ìƒì„± ë° ì €ì¥
            generated_vectors = []
            book_vecs_dir = os.path.join(vecs_output_dir, f"{file_name_no_ext}_vecs")
            os.makedirs(book_vecs_dir, exist_ok=True)

            for i, idx in enumerate(vec_selected_indices):
                chunk_text = vec_chunks[idx]
                vec_filename = f"{file_name_no_ext}_vec_{i+1:02d}.json"
                vec_path = os.path.join(book_vecs_dir, vec_filename)

                print(f"    -> Vectorizing chunk {idx+1}/{len(vec_chunks)}...", end="", flush=True)
                
                # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¡œë“œ (API ì ˆì•½)
                if os.path.exists(vec_path):
                    print(" (Loading existing)...", end="")
                    with open(vec_path, 'r', encoding='utf-8') as f:
                        vector = json.load(f)
                else:
                    vector = vectorizer.get_embedding(chunk_text)
                    if vector:
                        with open(vec_path, 'w', encoding='utf-8') as f:
                            json.dump(vector, f)
                
                if vector:
                    generated_vectors.append(vector)
                    print(" Done.")
                else:
                    print(" Failed.")
                
                time.sleep(0.2) # Rate limit

            # 4) í‰ê·  ë²¡í„° ê³„ì‚° ë° ì €ì¥
            if generated_vectors:
                avg_vector = vectorizer.get_average_embedding(generated_vectors)
                if avg_vector:
                    final_vec_filename = f"{file_name_no_ext}_vec_all.json"
                    final_vec_path = os.path.join(final_vec_output_dir, final_vec_filename)
                    
                    with open(final_vec_path, 'w', encoding='utf-8') as f:
                        json.dump(avg_vector, f)
                    print(f"    -> Final average vector saved to {final_vec_path}")
                else:
                    print("    âŒ Failed to calculate average vector.")
            else:
                print("    âŒ No vectors generated.")

        except Exception as e:
            print(f"  âŒ Failed to process vectors: {e}")

    print("\nâœ… All processing completed!")

if __name__ == "__main__":
    main()
